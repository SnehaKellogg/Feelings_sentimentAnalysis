{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "from textgenrnn import textgenrnn\n",
    "\n",
    "\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    cfg = yaml.load(f)\n",
    "\n",
    "\n",
    "texts = []\n",
    "context_labels = []\n",
    "\n",
    "for user in cfg['twitter_users']:\n",
    "    print(\"Downloading {}'s Tweets...\".format(user))\n",
    "    all_tweets = tweepy.Cursor(api.user_timeline,\n",
    "                               screen_name=user,\n",
    "                               count=200,\n",
    "                               tweet_mode='extended',\n",
    "                               include_rts=False).pages(16)\n",
    "    for page in all_tweets:\n",
    "        for tweet in page:\n",
    "            tweet_text = clean_tweet(tweet.full_text)\n",
    "            if tweet_text is not '':\n",
    "                texts.append(tweet_text)\n",
    "                context_labels.append(user)\n",
    "\n",
    "textgen = textgenrnn(name='{}_twitter'.format(\"_\".join(cfg['twitter_users'])))\n",
    "\n",
    "if cfg['new_model']:\n",
    "    textgen.train_new_model(\n",
    "        texts,\n",
    "        context_labels=context_labels,\n",
    "        num_epochs=cfg['num_epochs'],\n",
    "        gen_epochs=cfg['gen_epochs'],\n",
    "        batch_size=cfg['batch_size'],\n",
    "        train_size=cfg['train_size'],\n",
    "        rnn_layers=cfg['model_config']['rnn_layers'],\n",
    "        rnn_size=cfg['model_config']['rnn_size'],\n",
    "        rnn_bidirectional=cfg['model_config']['rnn_bidirectional'],\n",
    "        max_length=cfg['model_config']['max_length'],\n",
    "        dim_embeddings=cfg['model_config']['dim_embeddings'],\n",
    "        word_level=cfg['model_config']['word_level'])\n",
    "else:\n",
    "    textgen.train_on_texts(\n",
    "        texts,\n",
    "        context_labels=context_labels,\n",
    "        num_epochs=cfg['num_epochs'],\n",
    "        gen_epochs=cfg['gen_epochs'],\n",
    "        train_size=cfg['train_size'],\n",
    "        batch_size=cfg['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\victo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 4,025 character sequences.\n",
      "Epoch 1/1\n",
      "31/31 [==============================] - ETA: 2:12 - loss: 1.809 - ETA: 1:07 - loss: 1.939 - ETA: 46s - loss: 1.986 - ETA: 35s - loss: 1.92 - ETA: 28s - loss: 1.85 - ETA: 24s - loss: 1.77 - ETA: 20s - loss: 1.71 - ETA: 18s - loss: 1.75 - ETA: 16s - loss: 1.73 - ETA: 14s - loss: 1.78 - ETA: 13s - loss: 1.75 - ETA: 11s - loss: 1.73 - ETA: 10s - loss: 1.71 - ETA: 9s - loss: 1.6937 - ETA: 8s - loss: 1.674 - ETA: 8s - loss: 1.661 - ETA: 7s - loss: 1.640 - ETA: 6s - loss: 1.620 - ETA: 5s - loss: 1.619 - ETA: 5s - loss: 1.624 - ETA: 4s - loss: 1.611 - ETA: 4s - loss: 1.597 - ETA: 3s - loss: 1.591 - ETA: 3s - loss: 1.576 - ETA: 2s - loss: 1.559 - ETA: 2s - loss: 1.555 - ETA: 1s - loss: 1.545 - ETA: 1s - loss: 1.539 - ETA: 0s - loss: 1.522 - ETA: 0s - loss: 1.522 - 13s 412ms/step - loss: 1.5128\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "RT : why the fuck did tommy pickles  Pickles who the fuck did tommy pickles of pickles of the first pickles of the flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles  Pickles in the flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles  #Pickles of pickles of the fuck did tommy pickles of the flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "The the first most pickles of that should pickles\n",
      "\n",
      "RT : I' only pickle who whonus pickles of pickles to pickles, who the fuck did tommy pickles of as the flathead should pickles of that a flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles  Picklest broth pickles and did tommy pickles of back of stotagg is a flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I‚Äôm fcirl of grow of pickles from pickles\n",
      "\n",
      "sneeth pickles play frads be who looking athers. I'm for the firing hava people was a fis worline he done but pickles\n",
      "\n",
      "Low Polisher I know pickles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi this girls who did the flathead eid scotioning so that a whole mechanic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                        | 1/3 [00:01<00:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I therk of that pickles of pickles of this have a whole mechanic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 2/3 [00:02<00:01,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT : on some pickles pickles of that a whole mechanic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "textgen=textgenrnn()\n",
    "textgen.train_on_texts(list, num_epochs=1)\n",
    "textgen.generate(3, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen.train_new_model(\n",
    "        texts,\n",
    "        context_labels=context_labels,\n",
    "        num_epochs=cfg['num_epochs'],\n",
    "        gen_epochs=cfg['gen_epochs'],\n",
    "        batch_size=cfg['batch_size'],\n",
    "        train_size=cfg['train_size'],\n",
    "        rnn_layers=cfg['model_config']['rnn_layers'],\n",
    "        rnn_size=cfg['model_config']['rnn_size'],\n",
    "        rnn_bidirectional=cfg['model_config']['rnn_bidirectional'],\n",
    "        max_length=cfg['model_config']['max_length'],\n",
    "        dim_embeddings=cfg['model_config']['dim_embeddings'],\n",
    "        word_level=cfg['model_config']['word_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from twitterscraper.query import query_tweets\n",
    "from twitterscraper.query import query_user_info\n",
    "from twitterscraper.ts_logger import logger\n",
    "\n",
    "\n",
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj, '__json__'):\n",
    "            return obj.__json__()\n",
    "        elif isinstance(obj, collections.Iterable):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, dt.datetime):\n",
    "            return obj.isoformat()\n",
    "        elif hasattr(obj, '__getitem__') and hasattr(obj, 'keys'):\n",
    "            return dict(obj)\n",
    "        elif hasattr(obj, '__dict__'):\n",
    "            return {member: getattr(obj, member)\n",
    "                    for member in dir(obj)\n",
    "                    if not member.startswith('_') and\n",
    "                    not hasattr(getattr(obj, member), '__call__')}\n",
    "\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def valid_date(s):\n",
    "    try:\n",
    "        return dt.datetime.strptime(s, \"%Y-%m-%d\").date()\n",
    "    except ValueError:\n",
    "        msg = \"Not a valid date: '{0}'.\".format(s)\n",
    "\n",
    "def main(query, limit, begindate, enddate):\n",
    "#         parser.add_argument(\"--profiles\", action='store_true',\n",
    "#                             help=\"Set this flag to if you want to scrape profile info of all the users where you\" \n",
    "#                             \"have previously scraped from. After all of the tweets have been scraped it will start\"\n",
    "#                             \"a new process of scraping profile pages.\")\n",
    "#         parser.add_argument(\"--lang\", type=str, default=None,\n",
    "#                             help=\"Set this flag if you want to query tweets in \\na specific language. You can choose from:\\n\"\n",
    "#                                  \"en (English)\\nar (Arabic)\\nbn (Bengali)\\n\"\n",
    "#                                  \"cs (Czech)\\nda (Danish)\\nde (German)\\nel (Greek)\\nes (Spanish)\\n\"\n",
    "#                                  \"fa (Persian)\\nfi (Finnish)\\nfil (Filipino)\\nfr (French)\\n\"\n",
    "#                                  \"he (Hebrew)\\nhi (Hindi)\\nhu (Hungarian)\\n\"\n",
    "#                                  \"id (Indonesian)\\nit (Italian)\\nja (Japanese)\\n\"\n",
    "#                                  \"ko (Korean)\\nmsa (Malay)\\nnl (Dutch)\\n\"\n",
    "#                                  \"no (Norwegian)\\npl (Polish)\\npt (Portuguese)\\n\"\n",
    "#                                  \"ro (Romanian)\\nru (Russian)\\nsv (Swedish)\\n\"\n",
    "#                                  \"th (Thai)\\ntr (Turkish)\\nuk (Ukranian)\\n\"\n",
    "#                                  \"ur (Urdu)\\nvi (Vietnamese)\\n\"\n",
    "#                                  \"zh-cn (Chinese Simplified)\\n\"\n",
    "#                                  \"zh-tw (Chinese Traditional)\"\n",
    "#                                  )\n",
    "#             if args.profiles and tweets:\n",
    "#                 list_users = list(set([tweet.user for tweet in tweets]))\n",
    "#                 list_users_info = [query_user_info(elem) for elem in list_users]\n",
    "#                 filename = 'userprofiles_' + args.output\n",
    "#                 with open(filename, \"w\", encoding=\"utf-8\") as output:\n",
    "#                     json.dump(list_users_info, output, cls=JSONEncoder)\n",
    "    tweets = twitterscraper.query.query_tweets(query, begindate, enddate, limit, lang='en')\n",
    "    dictionary  = (json.dumps(tweets, cls=JSONEncoder))\n",
    "    df = pd.read_json(dictionary)\n",
    "    smaller_df = df[[\"text\", \"timestamp\", \"tweet_id\", \"user_id\", \"username\"]]\n",
    "    tweet_list = []\n",
    "    for row in smaller_df.itertuples():\n",
    "        #new_dict= {}\n",
    "        #new_dict['tweet_id'] = row.tweet_id\n",
    "        #new_dict['text'] = row.text\n",
    "        #new_dict['time'] = row.timestamp\n",
    "        #prob_dist = loaded_model.prob_classify(row.text)\n",
    "        #new_dict['score'] = prob_dist.prob('pos')\n",
    "        #username = row.username\n",
    "        #twitterscraper.query.quer_user_info\n",
    "        tweet_list.append(row.text)\n",
    "    return(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "main() missing 2 required positional arguments: 'begindate' and 'enddate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-3908fdbec046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m23\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: main() missing 2 required positional arguments: 'begindate' and 'enddate'"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "begin=dt.date(2019, 7, 17)\n",
    "end=dt.date(2019, 7, 23)\n",
    "tweets = main(\"pickle\", limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2019, 7, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.date(2019, 7, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end - begin).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitterscraper.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)   # Remove URLs\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)  # Remove @ mentions\n",
    "    text = text.strip(\" \")   # Remove whitespace resulting from above\n",
    "    text = re.sub(r' +', ' ', text)   # Remove redundant spaces\n",
    "\n",
    "    # Handle common HTML entities\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table = pd.read_csv(r'C:\\Users\\victo\\Desktop\\Github Repos\\project3\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "list =[]\n",
    "for row in table.itertuples():\n",
    "    process_tweet = process_tweet_text(row.text)\n",
    "    list.append(process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Someone bring me pickles',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : if you ever refer to pickles as ‚Äúgherkins‚Äù around me i will roundhouse kick you in the fucking jaw',\n",
       " 'RT : Dear pal #AngelicaMaria proves once again why she, \"Mexico\\'s Sweetheart\\' is a living legend. Her comic talents shine in‚Ä¶',\n",
       " 'Story continues üë®\\u200düç≥\\r\\n\\r\\n1. Traditional Polish Sourdough Baked in Fermented Cabbage\\r\\n2. Walnuts ice cream - clear love t‚Ä¶',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'I‚Äôm just a boy looking at a girl praying she doesn‚Äôt like pickles',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'I want to try these pickle sandwichesüò© the way I love pickles',\n",
       " \"RT : Hey there! I'm Pickles and I'm a B&W comic artist in need of work. I'm available for projects right now!\\r\\n\\r\\n‚ú®Portfolio/Resum‚Ä¶\",\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : I don‚Äôt trust people who like pickles',\n",
       " \"RT : #PortfolioDay Hi, I'm Pickles! I draw comics and stuff and love working in black and white. I'm available for freelance wo‚Ä¶\",\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'First batch of pickles of the season. Darned cucumber beetles may make this the last. Still...enjoying the garden a‚Ä¶',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " \"are the best. You will 100% not be disappointed and if you are... I'll eat 7 pickles.\",\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : Great news! One of the first problems should tackle is the implementation of all the r‚Ä¶',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'I got Didi Pickles!',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'And Stu Pickles is obviously Leland Palmer.',\n",
       " \"üç©\\r\\n\\r\\nHere are your Beal's Pickles & Pints Final Totals. #AllHandsOnDeck\",\n",
       " 'I‚Äôm about to go buy me a jar of pickles I can‚Äôt take it.',\n",
       " 'The only time I eat pickles',\n",
       " 'RT : I would destroy a mound of fried pickles right now.',\n",
       " 'My dad made pickles I wanna say yesterday',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'Ok but Wendy‚Äôs need some new pickles because them sweet ass pickles do not belong on a burger',\n",
       " 'RT : Imagine NOT liking pickles... pathetic. yalls taste buds are weak and low of class.',\n",
       " 'I understand him in regards of the pickles! I don‚Äôt like the smell and the taste of them !',\n",
       " 'We went to the farm to get pickling cucumbers for the garlic dill pickle class and I accidentally impulse purchased‚Ä¶',\n",
       " 'RT : Great news! One of the first problems should tackle is the implementation of all the r‚Ä¶',\n",
       " 'Time to make the pickles.',\n",
       " 'oh well thats just cruel then',\n",
       " 'that‚Äôs not even that bad bc I love both olives & pickles. I‚Äôd probably get sick of them after a week though',\n",
       " 'Of course the day after fair is the day that I now want to demolish some fair fries, friend pickles, cheese curds,‚Ä¶',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'That‚Äôs a lot of pickles.',\n",
       " 'In Britain we need the full implementation of all recommendations from the Sir Eric Pickles‚Ä¶',\n",
       " 'I went back for more fried pickles they so bomb lol',\n",
       " 'Does have enough pickles for his burger? I‚Äôm more shocked he neglected the ketchup he loves so much.',\n",
       " 'Yesterday I drove myself 40 minutes to a bar that makes their own pickles.... I imagine this i‚Ä¶',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic',\n",
       " 'RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweet(tweet_dict):\n",
    "    tweet_list =[]\n",
    "    for row in tweet_dict.itertuples():\n",
    "        process_tweet = process_tweet_text(row.text)\n",
    "        tweet_list.append(process_tweet)\n",
    "    textgen=textgenrnn()\n",
    "    textgen.train_on_texts(tweet_list, num_epochs=1)\n",
    "    generated_tweet = textgen.generate(1, temperature=0.5)\n",
    "    return(generated_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 4,025 character sequences.\n",
      "Epoch 1/1\n",
      "31/31 [==============================] - ETA: 2:18 - loss: 2.001 - ETA: 1:10 - loss: 2.157 - ETA: 48s - loss: 1.983 - ETA: 36s - loss: 1.92 - ETA: 29s - loss: 1.87 - ETA: 25s - loss: 1.80 - ETA: 21s - loss: 1.75 - ETA: 18s - loss: 1.74 - ETA: 16s - loss: 1.73 - ETA: 14s - loss: 1.68 - ETA: 13s - loss: 1.68 - ETA: 12s - loss: 1.69 - ETA: 11s - loss: 1.68 - ETA: 10s - loss: 1.65 - ETA: 9s - loss: 1.6277 - ETA: 8s - loss: 1.646 - ETA: 7s - loss: 1.632 - ETA: 6s - loss: 1.628 - ETA: 6s - loss: 1.630 - ETA: 5s - loss: 1.616 - ETA: 4s - loss: 1.608 - ETA: 4s - loss: 1.597 - ETA: 3s - loss: 1.588 - ETA: 3s - loss: 1.578 - ETA: 2s - loss: 1.586 - ETA: 2s - loss: 1.590 - ETA: 1s - loss: 1.573 - ETA: 1s - loss: 1.565 - ETA: 0s - loss: 1.553 - ETA: 0s - loss: 1.544 - 13s 420ms/step - loss: 1.5326\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "RT : why the fuck did tommy pickles have a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles have a flathead screwdriver. nigga was a whole mechanic\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "RT : why the fuck did tommy pickles was a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles in screwdriver and pickles should the screwdriver screwdriver have a lot! I'm the screwdriver. nigga was a whole mechanic\n",
      "\n",
      "RT : why the fuck did tommy pickles have a flathead comic pickles was a whole screwdriver. nigga was a whole mechanic\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "RT :  Berka: Trear not eye at my mechanic\n",
      "\n",
      "RAe‚Äôs playable monta the screwdriver loando perended to are all of me to have a lock\n",
      "\n",
      "RT : why tomay los a cour me chartlead : wooks batch (70 fream job) Grafted Pickles of news a home I of pickles\n",
      "\n",
      "RT Walke People pickles a whole mechanic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_tweet(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT : why the fuck did tommy pickles  #Pickles of the firste pickles of the flathead screwdriver. nigga was a whole mechanic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(1, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
